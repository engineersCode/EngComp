{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Content under Creative Commons Attribution license CC-BY 4.0, code under BSD 3-Clause License © 2017 L.A. Barba, N.C. Clementi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeing stats in a new light\n",
    "\n",
    "Welcome to the second lesson in \"Take off with stats,\" Module 2 of our course in _Engineering Computations_. In the previous lesson, [Cheers! Stats with Beers](http://go.gwu.edu/engcomp2lesson1), we did some exploratory data analysis with a data set of canned craft beers in the US [1]. We'll continue using that same data set here, but with a new focus on _visualizing statistics_.\n",
    "\n",
    "In her lecture [\"Looking at Data\"](https://youtu.be/QYDuAo9r1xE), Prof. Kristin Sainani says that you should always plot your data. Immediatly, several things can come to light: are there outliers in your data? (Outliers are data points that look abnormally far from other values in the sample.) Are there data points that don't make sense? (Errors in data entry can be spotted this way.) And especially, you want to get a _visual_ representation of how data are distributed in your sample.\n",
    "\n",
    "In this lesson, we'll play around with different ways of visualizing data. There are so many ways to play! Have a look at the gallery of [The Data Viz Project](http://datavizproject.com) by ferdio (a data viz agency in Copenhagen). Aren't those gorgeous? Wouldn't you like to be able to make some pretty pics like that? Python can help!\n",
    "\n",
    "Let's begin. We'll import our favorite Python libraries, and set some font parameters for our plots to look nicer. Then we'll load our data set for craft beers and begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "#Import rcParams to set font styles\n",
    "from matplotlib import rcParams\n",
    "\n",
    "#Set font style and size \n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the beers data set using pandas, and assign it to a dataframe\n",
    "beers = pandas.read_csv(\"../../data/beers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note:\n",
    "\n",
    "If you downloaded this notebook alone, and don't have the data file on the location we assume above, get it by adding a code cell, and execute the following code in it:\n",
    "\n",
    "```Python\n",
    "from urllib.request import urlretrieve\n",
    "URL = 'http://go.gwu.edu/engcomp2data1?accessType=DOWNLOAD'\n",
    "urlretrieve(URL, 'beers.csv')\n",
    "```\n",
    "The data file will be downloaded to your working directory, and you will then need to remove the path information, i.e., the string `'../../data/'`, in the code to read it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. Let's have a look at the first few rows of the `pandas` dataframe we just created from the file, and confirm that it's a dataframe using the `type()` function. We only display the first 10 rows to save some space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(beers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical vs. quantitative data\n",
    "\n",
    "As you can see in the nice table that `pandas` printed for the dataframe, we have several features for each beer: the label `abv` corresponds to the acohol-by-volume fraction, label `ibu` refers to the international bitterness unit (IBU), then we have the `name` of the beer and the `style`, the brewery ID number, and the liquid volume of the beer can, in ounces.\n",
    "\n",
    "Alcohol-by-volume is a numeric feature: a volume fraction, with possible values from 0 to 1 (sometimes also given as a percentage). In the first 10 rows of our dataframe, the `ibu` value is missing (all those `NaN`s), but we saw in the previous lesson that `ibu` is also a numeric feature, with values that go from a minimum of 4 to a maximum of 138 (in our data set). IBU is pretty mysterious: how do you measure the bitterness of beer? It turns out that bitterness is measured as parts per million of _isohumulone_, the acid found in hops [2]. Who knew?\n",
    "\n",
    "For these numeric features, we learned that we can get an idea of the _central tendency_ in the data using the **mean value**, and we get ideas of _spread_ of the data with the **standard deviation** (and also with the range, but standard deviation is the most common).\n",
    "\n",
    "Notice that the beer data also has a feature named `style`: it can be \"American IPA\" or \"American Porter\" or a bunch of other styles of beer. If we want to study the beers according to style, we'll have to come up with some new ideas, because you can't take the mean or standard deviation of this feature!\n",
    "\n",
    "**Quantitative data** have meaning through a numeric feature, either on a continuous scale (like a fraction from 0 to 1), or a discrete count. \n",
    "**Categorical data**, in contrast, have meaning through a qualitative feature (like the style of beer). Data in this form can be collected in groups (categories), and then we can count the number of data items in that group. For example, we could ask how many beers (in our set) are of the style \"American IPA,\" or ask how many beers we have in each style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing quantitative data\n",
    "\n",
    "In the previous lesson, we played around a bit with the `abv` and `ibu` columns of the dataframe `beers`. For each of these columns, we extracted it from the dataframe and saved it into a `pandas` series, then we used the `dropna()` method to get rid of null values. This \"clean\" data was our starting point for some exploratory data analysis, and for plotting the data distributions using **histograms**. Here, we will add a few more ingredients to our recipes for data exploration, and we'll learn about a new type of visualization: the **box plot**.\n",
    "\n",
    "Let's repeat here the process for extracting and cleaning the two series, and getting the values into NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeat cleaning values abv\n",
    "abv_series = beers['abv']\n",
    "abv_clean = abv_series.dropna()\n",
    "abv = abv_clean.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeat cleaning values ibu\n",
    "ibu_series = beers['ibu']\n",
    "ibu_clean = ibu_series.dropna()\n",
    "ibu = ibu_clean.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also repeat a histogram plot for the `abv` variable, but this time choose to plot just 10 bins (you'll see why in a moment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(6,4))\n",
    "pyplot.hist(abv, bins=10, color='#3498db', histtype='bar', edgecolor='white') \n",
    "pyplot.title('Alcohol by Volume (abv) \\n')\n",
    "pyplot.xlabel('abv')\n",
    "pyplot.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tell that the most frequent values of `abv` fall in the bin just above 0.05 (5% alcohol), and the bin below. The mean value of our data is 0.06, which happens to be within the top-frequency bin, but data is not always so neat (sometimes, extreme values weigh heavily on the mean). Note also that we have a _right skewed_ distribution, with higher-frequency bins occuring in the lower end of the range than in the higher end.\n",
    "\n",
    "If you played around with the bin sizes in the previous lesson, you might have noticed that with a lot of bins, it becomes harder to visually pick out the patterns in the data. But if you use too few bins, the plot is also unhelpful. What number of bins is just right? Well, it depends on your data, so you'll just have to experiment and use your best judgement.\n",
    "\n",
    "Let's learn a new trick. It turns out that `pandas` has built-in methods to make histograms directly from columns of a dataframe! (It uses Matplotlib internally for that.) The syntax is short and sweet:\n",
    "\n",
    "```\n",
    "dataframe.hist(column='label')\n",
    "```\n",
    "\n",
    "And `pandas` plots a pretty nice histogram without help. You can add optional parameters to set these to your liking; see the [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.hist.html). Check it out, and compare with our previous plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers.hist(column='abv');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one do you like better? Well, the `pandas` histogram took a lot fewer lines of code to create.  And it doesn't look bad at all. But we do have more fine-grained control with Matplotlib. Which method you choose in a real situation will just depend on the situation and your preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring quantitative data (continued)\n",
    "\n",
    "In the [previous lesson](http://go.gwu.edu/engcomp2lesson1), you learned how to compute the mean of the data using `numpy.mean()`. How easy is that? But then we wrote our own custom functions to compute variance or standard deviation. It shouldn't surprise you by now that there are also NumPy functions for that!\n",
    "\n",
    "\n",
    "##### Exercise:\n",
    "\n",
    "Go to the documentation of [`numpy.var()`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.var.html) and analyze if this function is computing the _sample variance_. \n",
    "\n",
    "**Hint**: Check what it says about the \"data degrees of freedom.\"\n",
    "\n",
    "If you did the reading, you might have noticed that, by default, the argument `ddof` in `numpy.var()` is set to zero. If we use the default option, then we are not really calculating the sample variance. Recall from the previous lesson that the **sample variance** is:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}     \n",
    "     \\text{var}_{sample} = \\frac{1}{N-1}\\sum_{i} (x_i - \\bar{x})^2\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Therefore, we need to be explicit about the division by $N-1$ when calling `numpy.var()`. How do we do that? We explicitly set `ddof` to `1`.  \n",
    "\n",
    "For example, to compute the sample variance for our `abv` variable, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_abv = numpy.var(abv, ddof=1)\n",
    "print(var_abv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the standard deviation by taking the square root of `var_abv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_abv = numpy.sqrt(var_abv)\n",
    "print(std_abv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering if there is a built-in function for the standard deviation in NumPy. Go on and search online and try to find something.\n",
    "\n",
    "**Spoiler alert!**\n",
    "You will. \n",
    "\n",
    "##### Exercise:\n",
    "\n",
    "1. Read the documentation about the NumPy standard deviation function, compute the standard deviation for `abv` using this function, and check that you obtained the same value than if you take the square root of the variance computed with NumPy.\n",
    "\n",
    "2. Compute the variance and standard deviation for the variable `ibu`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Median value\n",
    "\n",
    "So far, we've learned to characterize quantitative data using the mean, variance and standard deviation.\n",
    "\n",
    "If you watched Prof. Sainani's lecture [Describing Quantitative Data: Where is the center?](https://youtu.be/tQ5slNYRcC4) (recommended in our previous lesson), you'll recall that she also introduced the **median**: the middle value in the data, the value that separates your data set in half. (If there's an even number of data values, you take the average between the two middle values.)\n",
    "\n",
    "As you may anticipate, NumPy has a built-in function that computes the median, helpfully named [`numpy.median()`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.median.html). \n",
    "\n",
    "##### Exercise:\n",
    "\n",
    "Using NumPy, compute the median for our variables `abv` and `ibu`. Compare the median with the mean, and look at the histogram to locate where the values fall on the x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots\n",
    "\n",
    "Another handy way to visualize the distribution of quantitative data is using **box plots**. By \"distribution\" of the data, we mean some idea of the dataset's \"shape\": where is the center, what is the range, what is the variation in the data. \n",
    "Histograms are the most popular type of plots in exploratory data analysis. But check out box plots: they are easy to make with `pyplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.boxplot(abv, labels=['Alcohol by volume']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.boxplot(ibu, labels=['International bitterness unit']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on here? Obviously, there is a _box_: it represents 50% of the data in the middle of the data range, with the line across it (here, in orange) indicating the _median_. \n",
    "\n",
    "The bottom of the box is at the 25th _percentile_, while the top of the box is at the 75th percentile. In other words, the bottom 25% of the data falls below the box, and the top 25% of the data falls above the box. \n",
    "\n",
    "\n",
    "_Confused by percentiles?_\n",
    "The Nth percentile is the value below which N% of the observations fall. \n",
    "\n",
    "Recall the bell curve from our previous lesson: we said that 95% of the data falls at a distance $\\pm 2 \\sigma$ from the mean. This implies that 5% of the data (the rest) falls in the (symmetrical) tails, which in turn implies that the 2.5 percentile is at $-2\\sigma$ from the mean, and the 97.5 percentile is at $+2\\sigma$ from the mean.\n",
    "\n",
    "The percentiles 25, 50, and 75 are also named _quartiles_, since they divide the data into quarters. They are named first, second (median) and third quartile, respectively. \n",
    "\n",
    "OK, back to box plots. The height of the box—between the 25th and 75th percentile—is called the _interquartile range_ (IQR). Outside the box, you have two vertical lines—the so-called \"whiskers\" of the box plot, which used to be called \"box and whiskers plot\" [3]. \n",
    "\n",
    "The whiskers extend to the upper and lower extremes (short horizontal line), each at a distance $1.5\\times \\text{IQR}$ from the box. Any values beyond the upper and lower extremes are shown with a marker (here, small circles) and are an indication of outliers in the data.\n",
    "\n",
    "##### A bit of history:\n",
    "\n",
    "\"Box-and-whiskers\" plots were invented by John Tukey over 45 years ago. Tukey was a famous mathematician/statistician who is credited with coining the words _software_ and _bit_ [4]. He was active in the efforts to break the _Enigma_ code durig WWII, and worked at Bell Labs in the first surface-to-air guided missile (\"Nike\"). A classic 1947 work on early design of the electonic computer acknowledged Tukey: he designed the electronic circuit for computing addition. Tukey was also a long-time advisor for the US Census Bureau, and a consultant for the Educational Testing Service (ETS), among many other contributions [5].\n",
    "\n",
    "##### Note:\n",
    "\n",
    "Box plots are also drawn horizontally. Often, several box plots are drawn side-by-side with the purpose of comparing distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing categorical data\n",
    "\n",
    "The typical method of visualizing categorical data is using **bar plots**. These show visually the frequency of appearance of items in each category, or the proportion of data in each category. Suppose we wanted to know how many beers of the same style are in our data set. Remember: the _style_ of the beer is an example of _categorical data_. Let's extract the column with the style information from the `beers` dataframe, assign it to a variable named `style_series`, check the type of this variable, and view the first 10 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_series = beers['style']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(style_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_series[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already in the first 10 elements we see that we have two beers of the style \"American IPA,\" two beers of the style \"American Pale Ale (APA),\" but only one beer of the style \"Oatmeal Stout.\" The question is: how many beers of each style are contained in the whole series? \n",
    "\n",
    "Luckily, `pandas` has a built-in function to answer that question: [`series.value_counts()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html) (where `series` is the variable name of the `pandas` series you want the counts for). Let's try it on our `style_series`, and save the result in a new variable named `style_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_counts = style_series.value_counts()\n",
    "style_counts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(style_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(style_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `len()` function tells us that `style_counts` has 99 elements. That is, there are a total of 99 styles of beer in our data set. Wow, that's a lot!\n",
    "\n",
    "Notice that `value_counts()` returned the counts sorted in decreasing order: the most popular beer in our data set is \"American IPA\" with 424 entries in our data. The next-most popular beer is \"American Pale Ale (APA)\" with a lot fewer entries (245), and the counts decrease sharply after that. Naturally, we'd like to know how much more popular are the top-2 beers from the rest. Bar plot to the rescue! \n",
    "\n",
    "Below, we'll draw a horizontal bar plot directly with `pandas` (which uses Matplotlib internally) using the [`plot.barh()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.plot.barh.html) method for series. We'll only show the first 20 beers, because otherwise we'll get a huge plot. This plot gives us a clear visualization of the popularity ranking of beer styles in the US!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_counts[0:20].plot.barh(figsize=(10,8), color='#008367', edgecolor='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing multiple data\n",
    "\n",
    "These visualizations are really addictive! We're now getting ambitious: what if we wanted to show more than one feature, together on the same plot? What if we wanted to get insights about the relationship between two features through a multi-variable plot? \n",
    "\n",
    "For example, don't you want to know if the bitterness of beers is associated with the alcohol-by-volume fraction? We do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots\n",
    "\n",
    "Maybe we can do this: imagine a plot that has the alcohol-by-volume on the absissa, and the IBU value on the ordinate. For each beer, can can place a dot on this plot with its `abv` and `ibu` values as $(x, y)$ coordinates. This is called a **scatter plot**.\n",
    "\n",
    "We run into a bit of a problem, though. The way we handled the beer data above, we extracted the column for `abv` into a series, dropped the null entries, and saved the values into a NumPy array. We then repeated this process for the `ibu` column. Because a lot more `ibu` values are missing, we ended up with two arrays of different length: 2348 entries for the `abv` series, and 1405 entries for the `ibu` series. If we want to make a scatter plot with these two features, we'll need series (or arrays) of the same length.\n",
    "\n",
    "Let's instead clean the whole `beers` dataframe (which will completely remove any row that has a null entry), and _then_ extract the values of the two series into NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beers_clean = beers.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibu = beers_clean['ibu'].values\n",
    "len(ibu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abv = beers_clean['abv'].values\n",
    "len(abv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both arrays now have 1403 entries—not 1405 (the length of the clean `ibu` data), because two rows that had a non-null `ibu` value _did_ have a null `abv` value and were dropped.\n",
    "\n",
    "With the two arrays of the same length, we can now call the [`pyplot.scatter()`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.scatter.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(8,6))\n",
    "pyplot.scatter(abv, ibu, color='#3498db') \n",
    "pyplot.title('Scatter plot of alcohol-by-volume vs. IBU \\n')\n",
    "pyplot.xlabel('abv')\n",
    "pyplot.ylabel('IBU');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. That's a bit of a mess. Too many dots! But we do make out that the beers with low alcohol-by-volume tend to have low bitterness. For higher alcohol fraction, the beers can be anywhere on the bitterness scale: there's a lot of vertical spread on those dots to the right of the plot. \n",
    "\n",
    "An idea! What if the bitterness has something to do with _style_? Neither of us knows much about beer, so we have no clue. Could we explore this question with visualization? We found a way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bubble chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beers_styles = beers_clean.drop(['Unnamed: 0','name','brewery_id','ounces','id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers_styles[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataframe with only the numeric features `abv` and `ibu`, and the categorical feature `style`. We want to kow how many beers we have of each style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_counts = beers_styles['style'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_counts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(style_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(style_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 90 different styles, and the number of beers in each style appears on each row of `style_counts`. According to our data, the most popular style is the _American IPA_, with 301 beers in the data set corresponding to this style.\n",
    "\n",
    "What would be a way to characterize the _style_ of the beers, given the data we have? Within a style, each beer has a different value of `abv` and `ibu`, but we can use the _mean value_ of each feature to characterize the style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_means = beers_styles.groupby('style').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_means[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_counts = style_counts.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_counts[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have available some plotting methods in `pandas` (which is actually calling Matplotlib internally). One of these is a scatter plot: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_means.plot.scatter(figsize=(10,10), \n",
    "                           x='abv', y='ibu', s=style_counts, \n",
    "                           alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "nrof_labels = len(style_counts)\n",
    "colors = cm.viridis(style_counts.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_means.plot.scatter(figsize=(10,10), \n",
    "                           x='abv', y='ibu', s=style_counts*15, color=colors,\n",
    "                           alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = style_means.plot.scatter(figsize=(12,12), \n",
    "                           x='abv', y='ibu', s=style_counts*15, color=colors,\n",
    "                           alpha=0.3);\n",
    "\n",
    "for i, txt in enumerate(list(style_counts.index.values)):\n",
    "    if style_counts.values[i] > 65:\n",
    "        ax.annotate(txt, (style_means.abv.iloc[i],style_means.ibu.iloc[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Craft beer datatset](https://github.com/nickhould/craft-beers-dataset) by Jean-Nicholas Hould.\n",
    "2. [What's The Meaning Of IBU?](https://beerconnoisseur.com/articles/whats-meaning-ibu) by Jim Dykstra for The Beer Connoisseur (2015).\n",
    "3. 40 years of boxplots (2011). Hadley Wickham and Lisa Stryjewski, _Am. Statistician_. [PDF available](http://vita.had.co.nz/papers/boxplots.pdf)\n",
    "4. [John Wilder Tukey](https://www.britannica.com/biography/John-Wilder-Tukey), Encyclopædia Britannica.\n",
    "5. John W. Tukey: His life and professional contributions (2002). David R. Brillinger, _Ann. Statistics_. [PDF available](https://www.stat.berkeley.edu/~brill/Papers/life.pdf)\n",
    "\n",
    "### Recommended viewing\n",
    "\n",
    "From [\"Statistics in Medicine,\"](https://lagunita.stanford.edu/courses/Medicine/MedStats-SP/SelfPaced/about), a free course in Stanford Online by Prof. Kristin Sainani, we highly recommend that you watch this lecture:\n",
    "* [Looking at data](https://youtu.be/QYDuAo9r1xE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to load the notebook's style sheet, then ignore it\n",
    "from IPython.core.display import HTML\n",
    "css_file = '../../style/custom.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
